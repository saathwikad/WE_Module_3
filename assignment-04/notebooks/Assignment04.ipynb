{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmgKce6KpwGWBMujBrdUBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saathwikad/WE_Module_3/blob/main/assignment-04/notebooks/Assignment04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Generation - Assignment04"
      ],
      "metadata": {
        "id": "z1I1_K6h-_Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version - 1 - Chatgpt"
      ],
      "metadata": {
        "id": "PTlyFtKz_E-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corpus Generation - Cleaning(Tokenization)"
      ],
      "metadata": {
        "id": "Q49Q4a_CCiqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tVwMMxx-6h5",
        "outputId": "bb090b38-8ffc-48ea-8b54-e1d7e27841f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens:\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'a', 'stitch', 'in', 'time', 'saves', 'nine', 'all', 'that', 'glitters', 'is', 'not', 'gold', 'actions', 'speak', 'louder', 'than', 'words', 'practice', 'makes', 'perfect']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_corpus(text):\n",
        "    # Remove punctuation marks\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Split text into tokens\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Sample text corpus\n",
        "text_corpus = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "A stitch in time saves nine.\n",
        "All that glitters is not gold.\n",
        "Actions speak louder than words.\n",
        "Practice makes perfect.\n",
        "\"\"\"\n",
        "\n",
        "# Clean the text corpus\n",
        "cleaned_tokens = clean_corpus(text_corpus)\n",
        "\n",
        "# Print the cleaned tokens\n",
        "print(\"Cleaned Tokens:\")\n",
        "print(cleaned_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cleaned Tokens:\")\n",
        "print(cleaned_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9uOvik2ARY7",
        "outputId": "fa3f787e-4049-4006-c939-f0439d553c65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens:\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'a', 'stitch', 'in', 'time', 'saves', 'nine', 'all', 'that', 'glitters', 'is', 'not', 'gold', 'actions', 'speak', 'louder', 'than', 'words', 'practice', 'makes', 'perfect']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function with internal corpus. Tested on sample testcases given by GPT."
      ],
      "metadata": {
        "id": "tBA0sJtgC1XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_internal_corpus(start_words: list[str], chain_length: int, num_generated: int) -> str:\n",
        "    \"\"\"\n",
        "    Generate a sentence similar to the sample text corpus provided in the code.\n",
        "\n",
        "    Parameters:\n",
        "        start_words (list[str]): A list of starting words, exactly as long as the chain_length.\n",
        "        chain_length (int): The length of the Markov chain, determining the number of previous words considered for prediction.\n",
        "        num_generated (int): The number of words in the generated sentence.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated sentence.\n",
        "    \"\"\"\n",
        "    # Sample text corpus provided in the code\n",
        "    sample_corpus = \"\"\"\n",
        "    The quick brown fox jumps over the lazy dog.\n",
        "    A stitch in time saves nine.\n",
        "    All that glitters is not gold.\n",
        "    Actions speak louder than words.\n",
        "    Practice makes perfect.\n",
        "    \"\"\"\n",
        "\n",
        "    # Clean the text corpus\n",
        "    tokens = clean_corpus(sample_corpus)\n",
        "\n",
        "    try:\n",
        "        # Build the Markov chain\n",
        "        markov_chain = build_markov_chain(tokens, chain_length)\n",
        "\n",
        "        # Generate a sentence using the Markov chain and starting words\n",
        "        current_words = start_words.copy()\n",
        "        generated_sentence = list(current_words)\n",
        "\n",
        "        for _ in range(num_generated):\n",
        "            current_state = tuple(current_words)\n",
        "            if current_state in markov_chain:\n",
        "                next_word = max(markov_chain[current_state], key=markov_chain[current_state].get)\n",
        "                generated_sentence.append(next_word)\n",
        "                current_words = generated_sentence[-chain_length:]\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return ' '.join(generated_sentence)\n",
        "\n",
        "    except ValueError as e:\n",
        "        return f\"Error: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "# Test the generate_with_internal_corpus function\n",
        "start_words = [\"the\", \"quick\"]\n",
        "chain_length = 2\n",
        "num_generated = 10\n",
        "generated_sentence = generate_with_internal_corpus(start_words, chain_length, num_generated)\n",
        "print(\"Generated Sentence:\", generated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpGrrWqlAW9g",
        "outputId": "eba046b4-0931-4fba-9729-6864ee00e544"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: the quick brown fox jumps over the lazy dog a stitch in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te following four testcases are given by GPT."
      ],
      "metadata": {
        "id": "5W2dN6EsEV5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Case 1\n",
        "start_words = [\"the\", \"quick\"]\n",
        "chain_length = 2\n",
        "num_generated = 10\n",
        "generated_sentence = generate_with_internal_corpus(start_words, chain_length, num_generated)"
      ],
      "metadata": {
        "id": "t3NM2m4zDFyF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Sentence:\", generated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEtRKh02DMob",
        "outputId": "b99a9d55-341a-4755-d389-248c4a056edd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: the quick brown fox jumps over the lazy dog a stitch in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"practice\", \"makes\"]\n",
        "chain_length = 3\n",
        "num_generated = 7\n",
        "generated_sentence = generate_with_internal_corpus(start_words, chain_length, num_generated)"
      ],
      "metadata": {
        "id": "GqCZTnWVDOpk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Sentence:\", generated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG4FZXfvDOz9",
        "outputId": "457c180a-a0e7-4d83-e283-67f36409b36d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: practice makes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"the\"]\n",
        "chain_length = 0  # Invalid chain length (less than 1)\n",
        "num_generated = 5\n",
        "generated_sentence = generate_with_internal_corpus(start_words, chain_length, num_generated)"
      ],
      "metadata": {
        "id": "Nb5bB6OrDO9Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Sentence:\", generated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0lWfsGJDPHI",
        "outputId": "a35a1fe4-a699-4377-df7b-050c3ddab522"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = []\n",
        "chain_length = 2\n",
        "num_generated = 7\n",
        "generated_sentence = generate_with_internal_corpus(start_words, chain_length, num_generated)"
      ],
      "metadata": {
        "id": "lb9prLDHDc8q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Sentence:\", generated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18B45S7oDdAh",
        "outputId": "f4250f72-5089-4a9b-f35e-33f9f90055ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My testcases for GPT."
      ],
      "metadata": {
        "id": "UPnssvNZEbUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = [\"I\", \"am\", \"a\"]\n",
        "chain_length = 3\n",
        "num_generated = 10\n",
        "generated_sentence = generate_with_internal_corpus(start_words, chain_length, num_generated)"
      ],
      "metadata": {
        "id": "VVxza6HHEdgT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Sentence:\", generated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqm3GbXHFAqZ",
        "outputId": "b788274f-9620-41ce-f8f8-850f76703887"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: I am a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function handling exceptions, errors. Requires sample external corpus. Yet to be tested."
      ],
      "metadata": {
        "id": "liU5GefCCq4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_external_corpus(filename: str, start_words: list[str], chain_length: int, num_generated: int) -> str:\n",
        "    \"\"\"\n",
        "    Generate a sentence similar to the text contained in the specified file.\n",
        "\n",
        "    Parameters:\n",
        "        filename (str): The path to the file containing the text corpus.\n",
        "        start_words (list[str]): A list of starting words, exactly as long as the chain_length.\n",
        "        chain_length (int): The length of the Markov chain, determining the number of previous words considered for prediction.\n",
        "        num_generated (int): The number of words in the generated sentence.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated sentence.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read text from the file\n",
        "        with open(filename, 'r') as file:\n",
        "            text_corpus = file.read()\n",
        "\n",
        "        # Clean the text corpus\n",
        "        tokens = clean_corpus(text_corpus)\n",
        "\n",
        "        # Validate chain length\n",
        "        if chain_length <= 0:\n",
        "            raise ValueError(\"Chain length must be a positive integer.\")\n",
        "\n",
        "        # Validate starting words\n",
        "        if len(start_words) != chain_length:\n",
        "            raise ValueError(\"The number of starting words must be equal to the chain length.\")\n",
        "\n",
        "        # Build the Markov chain\n",
        "        markov_chain = build_markov_chain(tokens, chain_length)\n",
        "\n",
        "        # Generate a sentence using the Markov chain and starting words\n",
        "        current_words = start_words.copy()\n",
        "        generated_sentence = list(current_words)\n",
        "\n",
        "        for _ in range(num_generated):\n",
        "            current_state = tuple(current_words)\n",
        "            if current_state in markov_chain:\n",
        "                next_word = max(markov_chain[current_state], key=markov_chain[current_state].get)\n",
        "                generated_sentence.append(next_word)\n",
        "                current_words = generated_sentence[-chain_length:]\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return ' '.join(generated_sentence)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: File '{filename}' not found.\"\n",
        "    except ValueError as e:\n",
        "        return f\"Error: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "# Test the generate_with_external_corpus function\n",
        "filename = \"sample_corpus.txt\"  # Provide the path to the text corpus file\n",
        "start_words = [\"the\", \"quick\"]  # Provide the starting words\n",
        "chain_length = 2\n",
        "num_generated = 10\n",
        "generated_sentence = generate_with_external_corpus(filename, start_words, chain_length, num_generated)\n",
        "print(\"Generated Sentence:\", generated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipSyywPHAS40",
        "outputId": "e715ecdc-a2ba-45ba-ed75-c43759a113cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: Error: File 'sample_corpus.txt' not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QLfrPtpCRsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}